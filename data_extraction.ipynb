{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Scraping\n",
    "The aim of this script is the tweet extraction using [twitter][1] and [searchtweets][2] according to the needs of this project. To run the first part of this notebook, the only prerequisite is to create a Twitter developer account successfully. In the second part, it is required to have a premium developer account.\n",
    "\n",
    "Later in this project we are going to analyse these tweets in respect with time. Therefore, our primary concern is to aqcuire the population of tweets in a specific time period, while the frequency and the volume of the data are of minor importance.\n",
    "\n",
    "[1]: https://pypi.org/project/twitter/\n",
    "[2]: https://github.com/twitterdev/search-tweets-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Tweet Extraction from Profiles\n",
    "In this section we focus on the tweet acquisition from various profiles. Twitter module offers a coprehensive framework to work with a free developer account. However, we need to import the necessary packages.\n",
    "\n",
    "#### 1.1 Import Packages\n",
    "While we use pandas package to save our results in `.json` format, it is possible to use [json][1] package instead.\n",
    "\n",
    "[1]: https://docs.python.org/3/library/json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Account Authintication\n",
    "After we import the necessary packages, we can save our twitter developer credentials. Note that the following credentials are not eligible and they are presented for the sake of format example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN    = '479738713-91JwRgHsr7wmqLCJQIkmgE04TiaiJHoEKhozgOwx'\n",
    "ACCESS_SECRET   = 'cDPoalGJx2wD6iq7SmBSz9EG2o2zjhL1M6qRBjaB5HwVk'\n",
    "CONSUMER_KEY    = 'TMLKPLjFsIxhT9smhiyAKA14p'\n",
    "CONSUMER_SECRET = 'onJDsMZBfNy0SF7VMMX1SHY6HPrw5MuOqimkaAtQzHGdUVnJZq'\n",
    "\n",
    "t = twitter.Api(consumer_key=CONSUMER_KEY,\n",
    "                consumer_secret=CONSUMER_SECRET,\n",
    "                access_token_key=ACCESS_TOKEN,\n",
    "                access_token_secret=ACCESS_SECRET,\n",
    "                tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Extract Maximum ID\n",
    "After we enter to our developer credentials successfully, we are ready to extract the data we need. As we mentioned above, the goal is to acquire data from a *number* of profiles in a *certain* time period. The first can be achieved using a loop over the profile names of interest, while the second is achieved by retrieving the tweet ID at the desired end point of time. Below there is an example, which can be used to obtain the end point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = t.GetUserTimeline(screen_name='microsoft',count=100)\n",
    "print(example.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Extract our Data\n",
    "It's worth noting that the tweets are sorted from the most recent to the oldest. Also, sandbox (free) accounts have a cap of 100 tweets per request. However, whenever we set the limit to 100 the twitter module functions tend to skip tweets upon retrieval. Hence, we decided to set a limit on 50 to ensure that we get all the data we need.\n",
    "\n",
    "Now we are ready to run the algorithm to extract and save the profile tweets in `.json` format.\n",
    "\n",
    "*Note*: In this case we retrieve data from the start of 2018, so we can use the `str.endswith()` method to simplify the process. Although, it is suggested to use datetime module from datetime package and `datetime.strftime()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File < intel.json > written succefully\n",
      "File < amd.json > written succefully\n",
      "File < nvidia.json > written succefully\n"
     ]
    }
   ],
   "source": [
    "profile_names = ['intel', 'amd', 'nvidia']\n",
    "\n",
    "# delete data var in case we run\n",
    "# the algorithm more than once\n",
    "try:\n",
    "    del data\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# loops over profiles\n",
    "for name in profile_names:\n",
    "    \n",
    "    goal = True                  # def goal\n",
    "    myDict = dict()\n",
    "    max_id = 1148939992389079040 # starting tweet\n",
    "    goal_year = '2017'           # ending point\n",
    "\n",
    "    \n",
    "    # loops until desired year\n",
    "    while goal:\n",
    "        timeline = t.GetUserTimeline(screen_name=name,\n",
    "                                     exclude_replies=True,\n",
    "                                     max_id=max_id, count=50)\n",
    "\n",
    "        # creates a list of dictionaries\n",
    "        for tweet in timeline:\n",
    "            # lists hashtags\n",
    "            if tweet.hashtags:\n",
    "                if len(tweet.hashtags) > 1:\n",
    "                    hashtags = list()\n",
    "                    for tag in tweet.hashtags:\n",
    "                        hashtags.append(tag.text)\n",
    "                else:\n",
    "                    hashtags = tweet.hashtags[0].text\n",
    "            else:\n",
    "                hashtags = 'null'\n",
    "\n",
    "            # lists media type (e.g. photo, video)\n",
    "            if tweet.media:\n",
    "                media = True\n",
    "            else:\n",
    "                media = False\n",
    "\n",
    "            myDict.append({'id': tweet.id,\n",
    "                           'created_sec': tweet.created_at_in_seconds,\n",
    "                           'text': tweet.full_text,\n",
    "                           'hashtags': hashtags,\n",
    "                           'media': media,\n",
    "                           'retweets': tweet.retweet_count,\n",
    "                           'created_at': tweet.created_at\n",
    "                          })\n",
    "\n",
    "            goal = not tweet.created_at.endswith(goal_year)\n",
    "            max_id = tweet.id-1\n",
    "        \n",
    "    data = pd.DataFrame.from_dict(myDict)\n",
    "    file_name = '%s.json' % name\n",
    "    \n",
    "    data.to_json(orient='values').replace(\"\\'\",\"\")\n",
    "    data.to_json(file_name)\n",
    "    print('File <%s> is written succefully' % file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Tweet Extraction from custom Queries\n",
    "In this section we retrieve a series of tweets using predefined queries. In order to have full access to historical data, we needed to upgrade the developer's account to Premium. The Premium account may provide full access to tweets, but it imposes limitations in both *requests* and *tweet usage* per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Import Packages\n",
    "When we changed to a Premium account, we faced a couple of authintication problems with Twitter package. Hence, we decided to use the suggested-by-Twitter package searchtweets to extract the data. Similarly, pandas can be replaced as suggested in the first section. Finally, we import time package to put the system on sleep to have greater control in case of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import load_credentials, gen_rule_payload, ResultStream, collect_results\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Account Authintication\n",
    "In comparison with twitter, searchtweets supports YAML to store the developer credentials, which increases security. YAML is a Markup Language and its extension is `.yaml`. Below there is an example of how to enter your credentials in a YAML file. It's worth mentioning that if you use Jupyter, you can just create a `.txt` file and rename the extension to `.yaml`.\n",
    "\n",
    "\n",
    "```yaml\n",
    "search_tweets_api:\n",
    "    account_type: premium\n",
    "    endpoint: https://api.twitter.com/1.1/tweets/search/fullarchive/example.json\n",
    "    consumer_key: TMLKPLjFsIxhT9smhiyAKA14p\n",
    "    consumer_secret: onJDsMZBfNy0SF7VMMX1SHY6HPrw5MuOqimkaAtQzHGdUVnJZq\n",
    "```\n",
    "\n",
    "Now we are ready to come back to Jupyter and put our credentials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "premium_search_args = load_credentials(\"twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Queries & Format\n",
    "The query format follows Twitter's advanced search rules. It is important to be **EXTRA** careful with the queries. First, assess the total amount of requests for your queries by trying them directly in Twitter search. Otherwise, there is a chance to run out of requests this month.\n",
    "\n",
    "In this project we pull tweets from 111 financial accounts that include the keywords intc, intel, amd, nvda or nvidia under a specific timeframe. We assess the amount of requests by calculating a \"generous\" average of tweets per day using Twitter's search.\n",
    "\n",
    "However, let's see how we created the queries in first place. First, we retrieved the [top 10][1] accounts from NASDAQ, [top 100][2] from Forbes and then we added NASDAQ Twitter account as well. While NASDAQ's top list was short, Forbes list was retrieved using Python. We were quite lucky and the list was in an [HTML Table][3], so we used magic command ```%%html``` to get the table locally and we saved it in a `.txt` file. Then, the process of creating a list with the accounts of interest was really simple.\n",
    "\n",
    "[1]: https://www.nasdaq.com/article/10-twitter-feeds-investors-need-to-follow-cm522728\n",
    "[2]: https://www.forbes.com/sites/alapshah/2017/11/16/the-100-best-twitter-accounts-for-finance/#130a10347ea0\n",
    "[3]: #html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Keywords\n",
    "keywords = ['INTC','Intel','AMD','NVDA','Nvidia']\n",
    "\n",
    "# NASDAQ List\n",
    "accounts = ['FinancialTimes','business','cnbc',\n",
    "            'stockTwits','WSJMoneyBeat','stlouisfed',\n",
    "            'Carl_C_Icahn','NASDAQ','carney',\n",
    "            'CGasparino','ZacksResearch']\n",
    "\n",
    "# Load Forbes List with pandas\n",
    "forbes_lst = pd.read_html('top_100.txt', header=0)[0]['Twitter Handle'].to_list()\n",
    "\n",
    "# Merging Lists\n",
    "accounts = accounts+forbes_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to change the format to be readable by Twitter, so we defined a simple query handler function according to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_handler(var,prefix=str(),space=False):\n",
    "    if prefix:\n",
    "        prefix = prefix + ':'\n",
    "        \n",
    "    if isinstance(var,list):\n",
    "        result = str()\n",
    "        for i in var:\n",
    "            result += ' OR ' + prefix + i\n",
    "            \n",
    "        var = result[4:len(result)].join('()')\n",
    "    else:\n",
    "        var = prefix + var\n",
    "    if space:\n",
    "        return var+' '\n",
    "    else:\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is one more problem with search queries in Twitter. There is a character limitation, so we need to seperate our query into a list of queries. Below there is the algorithm we used to implement this limit per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(keywords+accounts) > 30:\n",
    "    queries = list()\n",
    "    i = 0; until = 30-len(keywords)\n",
    "    \n",
    "    while i <= len(accounts):\n",
    "\n",
    "        queries.append(query_handler(keywords,space=True)+query_handler(accounts[i:until],'from'))\n",
    "\n",
    "        if until+30-len(keywords) < len(accounts):\n",
    "            i += until+1\n",
    "            until = i+30-len(keywords)\n",
    "        else:\n",
    "            i += 1\n",
    "            until = len(accounts)\n",
    "            queries.append(query_handler(keywords,space=True)+query_handler(accounts[i:until],'from'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Extract our Data\n",
    "In comparison with twitter requests which are free, here there is a possibility to run out of requests if the algorithm stucks and keeps doing meaningless loops. Therefore, we try to mitigate this risk by saving every milestone in `.json` format and having several flow control and error handlers in our algorithm.\n",
    "\n",
    "The following cell includes the variables we need to set before we run the algorithm for first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "since     = \"2018-01-03\"\n",
    "to_date   = \"2019-07-09\"\n",
    "path      = 'tweet_news/'\n",
    "file_name = 'query1'\n",
    "df        = pd.DataFrame()\n",
    "\n",
    "stuck     = False\n",
    "until     = to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run our algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-10 11:45\n",
      "2018-03-29 21:37\n",
      "2018-01-03 13:21\n",
      "2018-01-03 13:21\n",
      "\n",
      "query < (INTC OR Intel OR AMD OR NVDA OR Nvidia) (from:FinancialTimes OR from:business OR from:cnbc OR from:stockTwits OR from:WSJMoneyBeat OR from:stlouisfed OR from:Carl_C_Icahn OR from:NASDAQ OR from:carney OR from:CGasparino OR from:ZacksResearch OR from:John_Hempton OR from:BarbarianCap OR from:muddywatersre OR from:AlderLaneeggs OR from:CitronResearch OR from:BrattleStCap OR from:KerrisdaleCap OR from:modestproposal1 OR from:marketfolly OR from:EventDrivenMgr OR from:ActivistShorts OR from:Carl_C_Icahn OR from:LongShortTrader) >:\tFinished \n",
      "File name:\t query1 \n",
      "Number of observations:\t 1234 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    \n",
    "    try: # flow control\n",
    "        if not stuck: # if not error\n",
    "            df = pd.DataFrame()\n",
    "            file_name = query.split()[0]\n",
    "            until = to_date\n",
    "        else: # if error - skip queries and start from checkpoint\n",
    "            if query is not stuck_query:\n",
    "                continue\n",
    "            else:\n",
    "                file_name = query.split()[0]\n",
    "                until = stuck_date\n",
    "    except: # if run for first time - stuck not defined\n",
    "        df = pd.DataFrame()\n",
    "        file_name = query.split()[0]\n",
    "        until = to_date\n",
    "\n",
    "    while since is not until: # query paging loop\n",
    "\n",
    "        # query rules\n",
    "        rule = gen_rule_payload(query,\n",
    "                        results_per_call=500,\n",
    "                        from_date=since,\n",
    "                        to_date=until)\n",
    "\n",
    "        try: # flow control\n",
    "\n",
    "            # tweet request\n",
    "            tweets = collect_results(rule,\n",
    "                             max_results=500,\n",
    "                             result_stream_args=premium_search_args)\n",
    "            stuck = False\n",
    "\n",
    "            for tweet in tweets: # import values to dataframe\n",
    "\n",
    "                if tweet.in_reply_to_user_id: # boolean if it is reply\n",
    "                    reply = True\n",
    "                else:\n",
    "                    reply = False\n",
    "\n",
    "                if tweet.lang == 'en': # boolean if it is english\n",
    "                    english = True\n",
    "                else:\n",
    "                    english = False\n",
    "\n",
    "                if tweet.media_urls: # list media_urls\n",
    "                    media_url = tweet.media_urls\n",
    "                else:\n",
    "                    media_url = None\n",
    "\n",
    "                if tweet.hashtags: # list hashtags\n",
    "                    hashtags = tweet.hashtags\n",
    "                else:\n",
    "                    hashtags = None\n",
    "\n",
    "                if tweet.user_mentions: # boolean if there are mentions\n",
    "                    mention = True\n",
    "                else:\n",
    "                    mention = False\n",
    "\n",
    "                date = tweet.created_at_datetime.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "                df = df.append({'id': tweet.id,\n",
    "                                'user': tweet.user_id,\n",
    "                                'created_sec': tweet.created_at_seconds,\n",
    "                                'text': tweet.all_text,\n",
    "                                'hashtags': hashtags,\n",
    "                                'english': english,\n",
    "                                'followers': tweet.follower_count,\n",
    "                                'favorite': tweet.favorite_count,\n",
    "                                'media': media_url,\n",
    "                                'retweets': tweet.retweet_count,\n",
    "                                'quotes': tweet.quote_count,\n",
    "                                'type': tweet.tweet_type,\n",
    "                                'date': date,\n",
    "                                'time': tweet.created_at_string[11:19],\n",
    "                                'full_text': tweet.all_text\n",
    "                               },ignore_index=True)\n",
    "\n",
    "            # write checkpoint\n",
    "            df.to_json(path+file_name+'.json')\n",
    "            if until != date:\n",
    "                until = date\n",
    "                print(until)\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(date)\n",
    "                break\n",
    "\n",
    "        except: # error handler\n",
    "            stuck = True\n",
    "            stuck_query = query\n",
    "            stuck_date = until\n",
    "            print('WARNING LOOP BROKEN ON DATE:\\t', stuck_date)\n",
    "            break\n",
    "\n",
    "    if stuck:\n",
    "        break\n",
    "\n",
    "    print('\\nquery <', query, '>:\\tFinished',\n",
    "          '\\nFile name:\\t', file_name,\n",
    "          '\\nNumber of observations:\\t', len(df),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "---\n",
    "### Appendix: HTML Table\n",
    "The original table is long, so we present a sample of the original table in Jupyter;\n",
    "\n",
    "<a id='html'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"900\"> \n",
       "    <tbody> \n",
       "        <tr> \n",
       "            <td><span style=\"font-weight: 400;\">Rank</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">Twitter Handle</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">Popularity Rating</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">Total Followers</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">% of Total Followers</span></td> \n",
       "        </tr> \n",
       "        <tr> \n",
       "            <td><span style=\"font-weight: 400;\">1</span></td> \n",
       "            <td><a href=\"https://twitter.com/John_Hempton\" target=\"_blank\" rel=\"nofollow noopener\" data-ga-track=\"ExternalLink:https://twitter.com/John_Hempton\"><span style=\"font-weight: 400;\" data-ga-track=\"ExternalLink:https://twitter.com/John_Hempton\">John_Hempton</span></a></td> \n",
       "            <td><span style=\"font-weight: 400;\">79</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">24,400</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">0.32%</span></td>\n",
       "        </tr>\n",
       "        <tr> \n",
       "            <td><span style=\"font-weight: 400;\">2</span></td> \n",
       "            <td><a href=\"https://twitter.com/BarbarianCap\" target=\"_blank\" rel=\"nofollow noopener\" data-ga-track=\"ExternalLink:https://twitter.com/BarbarianCap\"><span style=\"font-weight: 400;\" data-ga-track=\"ExternalLink:https://twitter.com/BarbarianCap\">BarbarianCap</span></a></td> \n",
       "            <td><span style=\"font-weight: 400;\">76</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">21,100</span></td> \n",
       "            <td><span style=\"font-weight: 400;\">0.36%</span></td> \n",
       "        </tr> \n",
       "    </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<table width=\"900\"> \n",
    "    <tbody> \n",
    "        <tr> \n",
    "            <td><span style=\"font-weight: 400;\">Rank</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">Twitter Handle</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">Popularity Rating</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">Total Followers</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">% of Total Followers</span></td> \n",
    "        </tr> \n",
    "        <tr> \n",
    "            <td><span style=\"font-weight: 400;\">1</span></td> \n",
    "            <td><a href=\"https://twitter.com/John_Hempton\" target=\"_blank\" rel=\"nofollow noopener\" data-ga-track=\"ExternalLink:https://twitter.com/John_Hempton\"><span style=\"font-weight: 400;\" data-ga-track=\"ExternalLink:https://twitter.com/John_Hempton\">John_Hempton</span></a></td> \n",
    "            <td><span style=\"font-weight: 400;\">79</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">24,400</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">0.32%</span></td>\n",
    "        </tr>\n",
    "        <tr> \n",
    "            <td><span style=\"font-weight: 400;\">2</span></td> \n",
    "            <td><a href=\"https://twitter.com/BarbarianCap\" target=\"_blank\" rel=\"nofollow noopener\" data-ga-track=\"ExternalLink:https://twitter.com/BarbarianCap\"><span style=\"font-weight: 400;\" data-ga-track=\"ExternalLink:https://twitter.com/BarbarianCap\">BarbarianCap</span></a></td> \n",
    "            <td><span style=\"font-weight: 400;\">76</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">21,100</span></td> \n",
    "            <td><span style=\"font-weight: 400;\">0.36%</span></td> \n",
    "        </tr> \n",
    "    </tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
